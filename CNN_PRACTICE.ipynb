{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKuR+TKSUdIv2kHEY+3w8O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanu00007/Deep_Learning/blob/main/CNN_PRACTICE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P0WjOPpq3993"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert MNIST Image Files into tensor of 4-Dimensions (# of images ,Height,Width,color channel)\n",
        "transforms = transforms.ToTensor()\n",
        "train_data = datasets.MNIST(root='/data',train=True,download=True,transform=transforms)\n",
        "test_data = datasets.MNIST(root='/data',train=False,download=True,transform=transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUzjbpBF5HO2",
        "outputId": "e8b0b34d-98c4-461d-905b-25a86b6c1025"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 157058981.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/MNIST/raw/train-images-idx3-ubyte.gz to /data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 47616231.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/MNIST/raw/train-labels-idx1-ubyte.gz to /data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 43892028.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/MNIST/raw/t10k-images-idx3-ubyte.gz to /data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7605001.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CSXB70o54_0",
        "outputId": "5b0e7a21-845b-4ef9-be76-3b62eceec7b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: /data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GU55OdP7J9y",
        "outputId": "031a0892-aae1-48f6-8ebb-40210aaba565"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: /data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a small batch size for images.....lets say 10\n",
        "train_loader = DataLoader(train_data,batch_size=10,shuffle = True)\n",
        "test_loader = DataLoader(test_data,batch_size=10,shuffle = False)"
      ],
      "metadata": {
        "id": "vHIN9tXI7tME"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define our cnn model\n",
        "#describe convolution layer ans there are 2 layer\n",
        "\n",
        "#just an example\n",
        "\n",
        "conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=3,stride=1)\n",
        "conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=3,stride=1) #6 output from conv1 are input of conv2"
      ],
      "metadata": {
        "id": "SdnTdGK_d29g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grab 1 MNIST record/image\n",
        "for i,(X_train,y_train) in enumerate(train_data):\n",
        "    break"
      ],
      "metadata": {
        "id": "ne42NaWxeWW-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "3_faIZh8e81I",
        "outputId": "031336a4-144a-4c9a-e4d3-f718e1e14cd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = X_train.view(1,1,28,28) #4Dimension"
      ],
      "metadata": {
        "id": "UClCUmzefBGK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform out first convolution\n",
        "x = F.relu(conv1(x)) #Rectify Linear unit for our activation fuction\n",
        "# 1 single image, 6 is filter we asked for, 26x26\n",
        "x.shape\n"
      ],
      "metadata": {
        "id": "ys-xTbhpfG2q",
        "outputId": "03441c99-ad29-48d7-d2cf-d1db861552e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 26, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pooling layer\n",
        "x = F.max_pool2d(x,2,2) #max_pool\n",
        "x.shape\n",
        "\n",
        "#since pooling is 2x2 it change to 13x13 from 26x26"
      ],
      "metadata": {
        "id": "K_rHnIYnfO1v",
        "outputId": "1b5686d3-260c-4ddc-cf4a-e8dbc65be3b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 13, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Do our second convolution ayer\n",
        "x = F.relu(conv2(x))\n",
        "x.shape"
      ],
      "metadata": {
        "id": "ogaLNlc1gb3Q",
        "outputId": "cad5f753-58d0-45f1-f937-e84f7f412e95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 11, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = F.max_pool2d(x,2,2)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "ZxFqiRMOg5Op",
        "outputId": "f605fc04-4b94-4d76-fa88-1684349ed8da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "((28-2)/2-2)/2"
      ],
      "metadata": {
        "id": "YvojqkKwg9U6",
        "outputId": "afb1abe9-5939-4288-f327-14919b4704ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.5"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Class\n",
        "class ConvolutionalNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=3,stride=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=3,stride=1)\n",
        "        #Fully Conected layer\n",
        "        self.fc1 = nn.Linear(in_features=16*5*5,out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120,out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84,out_features=10)\n",
        "\n",
        "    def forward(self,X):\n",
        "      #first pass\n",
        "      X = F.relu(self.conv1(X))\n",
        "      X = F.max_pool2d(X,2,2)\n",
        "      #second pass\n",
        "      X = F.relu(self.conv2(X))\n",
        "      X = F.max_pool2d(X,2,2)\n",
        "\n",
        "      X = X.view(-1,16*5*5)\n",
        "      #Fully connected layer\n",
        "      X = F.relu(self.fc1(X))\n",
        "      X = F.relu(self.fc2(X))\n",
        "      X = self.fc3(X)\n",
        "      return F.log_softmax(X,dim=1)"
      ],
      "metadata": {
        "id": "znuuD8V_hc8T"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instance of our model\n",
        "torch.manual_seed(41)\n",
        "model = ConvolutionalNetwork()\n",
        "model"
      ],
      "metadata": {
        "id": "G06ZcFnUiBq2",
        "outputId": "0c140495-e50e-46d3-8bf0-c697db16edde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvolutionalNetwork(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Function optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001) #smaller the learning rate, longer its gonna take to train\n",
        "\n"
      ],
      "metadata": {
        "id": "1e3Jy3INjpDm"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#Create variables to track things\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "#Train our model\n",
        "for i in range(epochs):\n",
        "  trn_corr = 0\n",
        "  tst_corr = 0\n",
        "\n",
        "  #Train\n",
        "  for b,(X_train,y_train) in enumerate(train_loader):\n",
        "    b+=1 #start out batches ar 1\n",
        "    #Apply model\n",
        "    y_pred = model(X_train) #Not flattended\n",
        "    loss = criterion(y_pred,y_train)\n",
        "    predicted = torch.max(y_pred.data,1)[1] #add up the number of current predictions\n",
        "    batch_corr = (predicted == y_train).sum() # how many we got correct from specfic batch\n",
        "    trn_corr += batch_corr\n",
        "\n",
        "    #Backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #Print out some results\n",
        "    if b%600 == 0: #so each batch is 600\n",
        "      print(f'epoch:{i} bath: {b}  loss:{loss.item()}')\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corr)\n",
        "\n",
        "\n",
        "#Test\n",
        "with torch.no_grad(): #NO gradient so we dont update\n",
        "  for b,(X_test,y_test) in enumerate(test_loader):\n",
        "    y_val = model(X_test)\n",
        "    predicted = torch.max(y_val.data,1)[1] # adding up correct prediction\n",
        "    tst_corr += (predicted == y_test).sum()\n",
        "\n",
        "  loss = criterion(y_val,y_test)\n",
        "  test_losses.append(loss)\n",
        "  test_correct.append(tst_corr)\n",
        "\n",
        "\n",
        "\n",
        "current_time = time.time()\n",
        "print(f'Training took:{(current_time-start_time)/60} minutes')\n"
      ],
      "metadata": {
        "id": "awupXU8Pj37B",
        "outputId": "6881dc7f-d7f8-41b6-a9ff-0fde15833fff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 bath: 600  loss:0.1623610556125641\n",
            "epoch:0 bath: 1200  loss:0.1502392590045929\n",
            "epoch:0 bath: 1800  loss:0.4744560718536377\n",
            "epoch:0 bath: 2400  loss:0.14238706231117249\n",
            "epoch:0 bath: 3000  loss:0.007758188061416149\n",
            "epoch:0 bath: 3600  loss:0.3836284875869751\n",
            "epoch:0 bath: 4200  loss:0.0038223876617848873\n",
            "epoch:0 bath: 4800  loss:0.0021286322735249996\n",
            "epoch:0 bath: 5400  loss:0.0569545142352581\n",
            "epoch:0 bath: 6000  loss:0.00038789428072050214\n",
            "epoch:1 bath: 600  loss:0.06289136409759521\n",
            "epoch:1 bath: 1200  loss:0.010614877566695213\n",
            "epoch:1 bath: 1800  loss:0.03243611007928848\n",
            "epoch:1 bath: 2400  loss:0.012448625639081001\n",
            "epoch:1 bath: 3000  loss:0.000640809943433851\n",
            "epoch:1 bath: 3600  loss:0.0020938280504196882\n",
            "epoch:1 bath: 4200  loss:0.3140248656272888\n",
            "epoch:1 bath: 4800  loss:0.020231451839208603\n",
            "epoch:1 bath: 5400  loss:0.0031914091669023037\n",
            "epoch:1 bath: 6000  loss:0.0009488927898928523\n",
            "epoch:2 bath: 600  loss:0.04242878407239914\n",
            "epoch:2 bath: 1200  loss:0.000786997377872467\n",
            "epoch:2 bath: 1800  loss:0.0004456916940398514\n",
            "epoch:2 bath: 2400  loss:0.0021735907066613436\n",
            "epoch:2 bath: 3000  loss:6.313459743978456e-05\n",
            "epoch:2 bath: 3600  loss:0.0014708992093801498\n",
            "epoch:2 bath: 4200  loss:8.743518264964223e-05\n",
            "epoch:2 bath: 4800  loss:0.0011174092069268227\n",
            "epoch:2 bath: 5400  loss:0.00019280910782981664\n",
            "epoch:2 bath: 6000  loss:0.00013335500261746347\n",
            "epoch:3 bath: 600  loss:0.20401248335838318\n",
            "epoch:3 bath: 1200  loss:0.0011065044673159719\n",
            "epoch:3 bath: 1800  loss:0.00048520020209252834\n",
            "epoch:3 bath: 2400  loss:0.0013440614566206932\n",
            "epoch:3 bath: 3000  loss:0.00839989073574543\n",
            "epoch:3 bath: 3600  loss:9.791700722416863e-05\n",
            "epoch:3 bath: 4200  loss:0.0046706534922122955\n",
            "epoch:3 bath: 4800  loss:0.002486269222572446\n",
            "epoch:3 bath: 5400  loss:0.03304066136479378\n",
            "epoch:3 bath: 6000  loss:0.0277620367705822\n",
            "epoch:4 bath: 600  loss:0.04214929789304733\n",
            "epoch:4 bath: 1200  loss:0.0015658453339710832\n",
            "epoch:4 bath: 1800  loss:4.677330798585899e-05\n",
            "epoch:4 bath: 2400  loss:0.00042244786163792014\n",
            "epoch:4 bath: 3000  loss:0.554036021232605\n",
            "epoch:4 bath: 3600  loss:0.00038964804843999445\n",
            "epoch:4 bath: 4200  loss:0.027548160403966904\n",
            "epoch:4 bath: 4800  loss:0.004602053668349981\n",
            "epoch:4 bath: 5400  loss:0.05360576510429382\n",
            "epoch:4 bath: 6000  loss:0.03545317426323891\n",
            "Training took:3.2810540795326233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTDVGPcllDD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}